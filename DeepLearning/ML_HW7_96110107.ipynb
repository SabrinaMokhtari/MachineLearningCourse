{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML_HW7_96110107.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvoJvAyI2o-1"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.utils import save_image\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmochErpXll2"
      },
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "در ابتدا با مراجعه به این [لینک](https://towardsdatascience.com/downloading-kaggle-datasets-directly-into-google-colab-c8f0f407d73a) داده مدنظر را از kaggle دریافت و در گوگل کولب بارگذاری می کنیم. با mount کردن درایو می توانیم از داده دانلود و از زیپ خارج شده در کولب استفاده کنیم.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMbv3eEy6x43",
        "outputId": "ce6ff346-7e8a-4c2c-bb75-d19db1863cb1"
      },
      "source": [
        "# Loading data\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJu27mk97J8c"
      },
      "source": [
        "# Import OS for navigation and environment set up\n",
        "import os\n",
        "# Check current location, '/content' is the Colab virtual machine\n",
        "os.getcwd()\n",
        "# Enable the Kaggle environment, use the path to the directory your Kaggle API JSON is stored in\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/My Drive/ML/Pytorch/kaggle'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1YIUh5o3RXc",
        "outputId": "d3326cc2-5462-493c-d88f-3765c4113209"
      },
      "source": [
        "!pip install kaggle\n",
        "# kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "# Navigate into Drive where you want to store your Kaggle data\n",
        "os.chdir('/content/gdrive/My Drive/ML/Pytorch/kaggle')\n",
        "# Paste and run the copied API command, the data will download to the current directory\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "# Check contents of directory, you should see the .zip file for the competition in your Drive\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Downloading chest-xray-pneumonia.zip to /content/gdrive/My Drive/ML/Pytorch/kaggle\n",
            "100% 2.29G/2.29G [00:31<00:00, 16.8MB/s]\n",
            "100% 2.29G/2.29G [00:31<00:00, 78.4MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kaggle.json', 'chest-xray-pneumonia.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny0lUNLw5U8r",
        "outputId": "23912ce6-e4d9-40c1-f103-918481024c95"
      },
      "source": [
        "# Complete path to storage location of the .zip file of data\n",
        "zip_path = '/content/gdrive/My Drive/ML/Pytorch/kaggle/chest-xray-pneumonia.zip'\n",
        "# Check current directory (be sure you're in the directory where Colab operates: '/content')\n",
        "os.getcwd()\n",
        "# Copy the .zip file into the present directory\n",
        "!cp '{zip_path}' .\n",
        "# Unzip quietly \n",
        "!unzip -q 'chest-xray-pneumonia.zip'\n",
        "# View the unzipped contents in the virtual machine\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'chest-xray-pneumonia.zip', 'chest_xray', 'gdrive', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ic9HnodFGgA"
      },
      "source": [
        "main_dir = '/content/gdrive/My Drive/ML/Pytorch/kaggle/chest_xray'\n",
        "train_dir = main_dir + '/train'\n",
        "# train_data = torchvision.datasets.ImageFolder(train_dir, transform=T.Compose([T.Resize((150,150)), T.ToTensor()]))\n",
        "\n",
        "test_dir = main_dir + '/test'\n",
        "# test_data = torchvision.datasets.ImageFolder(test_dir, transform=T.Compose([T.Resize((150,150)), T.ToTensor()]))\n",
        "\n",
        "val_dir = main_dir + '/val'\n",
        "# val_data = torchvision.datasets.ImageFolder(val_dir, transform=T.Compose([T.Resize((150,150)), T.ToTensor()]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJMPANVaTZB4",
        "outputId": "ef0f3f84-02b4-4a31-a2b8-027b73171980"
      },
      "source": [
        "num_workers = 4\n",
        "batch_size = 128\n",
        "n_epochs = 20\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Ayf4-wV516"
      },
      "source": [
        "<div dir=rtl align=justify>\n",
        "\n",
        "\n",
        "*   باید Dataset مدنظر را طراحی کنیم. جهت ساخت custom dataset مدنظر توابعی رو به رو را override می کنیم __init__ ، __getitem__ ,__len__. در دیتاست مدنظر از قابلیت های ImageFolder در پایتورچ استفاده می کنیم تا با استفاده از پوشه های تعبیه شده در پوشه حاوی دیتاست، داده مربوط به آموزش، ولیدیشن و تست را ذخیره کنیم\n",
        "*   پس از این مرحله Dataloder را می سازیم تا batch های مدنظر را طراحی کنیم. batch را 128، تعداد worker هارا 4 و shuffle را true قرار می دهیم. با توجه به خواسته ی سوال از data augmentation در دیتا ست طراحی شده استفاده شده است. در 50% مواقع این transformation صورت می گیرد و در هر بار به احتمال 70% هریک از این 3 transformation روی داده اعمال می گردد. Data augmentation استراتژی ای است که این امکان را می دهد تا بدون جمع آوری داده های جدید، به طور قابل توجهی تنوع داده های موجود برای مدل های آموزشی را افزایش دهند. برای آموزش شبکه های عصبی بزرگ معمولاً از تکنیک های افزایش داده مانند cropping, Random Grayscale, horizontal flipping، Color Jitter و … استفاده می شود. با وارد کردن تغییرات تصادفی در تصویر اصلی، تنوع را افزایش می دهد، اما تعداد تصاویر / نمونه های مجموعه داده را افزایش نمی دهد. که در این بین از Resize، horizontal flippint، Grayscale و Color Jitter استفاده شد.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh-J8ppGRABc"
      },
      "source": [
        "# Prepare Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si-T2po2kpyG"
      },
      "source": [
        "\n",
        "**Transforms**\n",
        "\n",
        "1.   transforms.ColorJitter\n",
        "\n",
        "\n",
        "> Randomly change the brightness, contrast, saturation and hue of an image. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions. If img is PIL Image, mode “1”, “L”, “I”, “F” and modes with transparency (alpha channel) are not supported.\n",
        "\n",
        "\n",
        "2.   transforms.RandomHorizontalFlip\n",
        "\n",
        "\n",
        "> Horizontally flip the given image randomly with a given probability. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n",
        "\n",
        "\n",
        "3.   transforms.RandomGrayscale\n",
        "\n",
        "\n",
        "> Randomly convert image to grayscale with a probability of p (default 0.1). If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpUnRDTfwuYE"
      },
      "source": [
        "import random\n",
        "\n",
        "class MyImageDataset(Dataset):\n",
        "  def __init__(self, root, transform=None, target_transform=None, augment=None):\n",
        "    self.root = root\n",
        "    self.augment = augment\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.image_folder = torchvision.datasets.ImageFolder(self.root)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_folder.imgs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path, target = self.image_folder.imgs[idx]\n",
        "    img = self.image_folder.loader(path)\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "    if self.augment is not None:\n",
        "      r = random.uniform(0,1)\n",
        "      if r > 0.5:\n",
        "        r1 = random.uniform(0,1)\n",
        "        r2 = random.uniform(0,1)\n",
        "        r3 = random.uniform(0,1)\n",
        "        if r1 > 0.7:\n",
        "          img = self.augment[0](img)\n",
        "        if r2 > 0.7:\n",
        "          img = self.augment[1](img)\n",
        "        if r3 > 0.7:\n",
        "          img = self.augment[2](img)\n",
        "    return img, target\n",
        "\n",
        "trans=T.Compose(transforms=[T.Resize((216,216)), T.ToTensor()])\n",
        "augs=[T.RandomHorizontalFlip(), T.RandomGrayscale(), T.ColorJitter()]\n",
        "train_ds = MyImageDataset(root=train_dir, transform=trans, augment=augs)\n",
        "test_ds = MyImageDataset(root=test_dir, transform=trans)\n",
        "val_ds = MyImageDataset(root=val_dir, transform=trans)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH4XGfEwSZAu",
        "outputId": "0319b84c-5f80-4f02-b7b3-977d2e3bccea"
      },
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers = num_workers, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers = num_workers, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, num_workers = num_workers, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-pfIxK_YHM1"
      },
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "پس از این مرحله مدل را طراحی می کنیم. مدل از 2 بخش تشکیل می شود:\n",
        "\n",
        "*   ابتدا بلاک های مدنظر را طراحی می کنیم که طبق معماری گفته شده دارای دو لایه convolution و بلافاصله بعد آن batch normalization و سپس activation function می باشد. همچنین برای پیاده سازی skip connection از conv استفاده کرده. Conv مربوط به لایه اول و skip دارای stride 3 و دیگر لایه conv دارای stride با مقدار 1 می باشد. با توجه به این اعداد مقدار padding را برای convolution با سایز فیلتر 7 برابر 3، برای فیلتر با سایز 5 برابر 2 و برای فیلتر با سایز 3 برابر 1 قرار داده و convolution مربوط به لایه skip را برابر 1 قرار داده تا با توجه به سایز فیلتر های گفته شده به مشکلی بر نخوریم. در صورتی که میزان stride برابر 1 نباشد یا به به هرشکلی تعداد کانال های خروجی و ورودی برابر نباشد باید از convolution برای تغییر بعد x ورودی استفاده کنیم تا مرحله skip به درستی انجام بگیرد. در این حالت میزان stride همانطور که گفته شد برابر 3 و میزان padding برابر 0 و اندازه فیلتر برابر 1 می باشد. \n",
        "\n",
        "*   سپس باید resnet نهایی را پیاده سازی کنیم که از 3 بلوک فوق با سایز فیلتر به ترتیب 7 و5 و3 تشکیل شده است. و در نهایت یک لایه fully connected که در انتها قرار می گیرد.\n",
        "\n",
        "در ادامه این مراحل نشان داده شده است:\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVuxtKwDUDeL"
      },
      "source": [
        "# Creating Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQVGk18pBDUB"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=3, kernel_size=7):\n",
        "      super(Block, self).__init__()\n",
        "\n",
        "      self.skip = nn.Sequential()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.stride = stride\n",
        "\n",
        "      if stride != 1 or in_channels != out_channels:\n",
        "        self.skip = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=self.stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels))\n",
        "      else:\n",
        "        self.skip = None\n",
        "\n",
        "      self.block = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=self.kernel_size, padding=(self.kernel_size - 1)//2, stride=self.stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=self.kernel_size, padding=(self.kernel_size - 1)//2, stride=1, bias=False),\n",
        "          nn.BatchNorm2d(out_channels))\n",
        "      self.relu = nn.ReLU()\n",
        "        \n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "    out = self.block(x)\n",
        "\n",
        "    if self.skip is not None:\n",
        "        identity = self.skip(x)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMJdLzWhQ8KT"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.image_channels = 3\n",
        "        self.conv1 = nn.Conv2d(self.image_channels, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer1 = Block(3, 64, 3, 7)\n",
        "        self.layer2 = Block(64, 128, 3, 5)\n",
        "        self.layer3 = Block(128, 256, 3, 3)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(256 * 64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        x = F.softmax(x, dim=0)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8R-bJrbYmVZ"
      },
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "* در ادامه باید مراحل آموزش طی بشود. این روند طی 25 ایپاک و iteration های تعریف شده طبق سایز batch ها انجام می گیرد. \n",
        "\n",
        "* برای اینکار از Adam optimizer استفاده می کنیم. Lr در ابتدا 0.005 می باشد و در هر 5 ایپاک با استفاده از scheduler پایتورچ این مقدار کاهش پیدا می کند. زمانی که lr در ابتدا ثابت بود میزان train loss روی 0.2 به طور متوسط ثابت می ماند و تغییری شاهد نبودیم بنابراین از روش کاهش lr در هر 5 ایپاک استفاده کردیم و باعث شد میزان loss تا 0.007 به طور متوسط کاهش پیدا کند. \n",
        "\n",
        "* در هر 5 ایپاک میزان validation loss نیز روی داده های validation تست می شد. و نتایج آن نیز در ادامه مشاهده می شود.\n",
        "\n",
        "* در ابتدا drop out در نظر گرفته نمی شد و باعث شد میزان accuracy برای داده های تست 65% باشد درحالیکه برای داده آموزش تقریبا overfit میشد. بنابراین طبق مقالات ارائه شده در ارتباط با نقطه ای که از dropout استفاده میشود، در CONV->RELU->DROP از آن استفاده کرده و احتمال آن را طبق خواسته ی سوال 0.4 در نظر گرفته شد. در این حالت دقت روی داده ی تست تا حدود 75% افزایش پیدا کرد. \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Htd0RKb3UTS"
      },
      "source": [
        "# Instantiating Model, Optimizer and Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6YVqwnW1D4i"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = ResNet()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.005)\n",
        "# decayRate = 0.96\n",
        "my_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn5-SRdQugna"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHEEzvqiujaJ",
        "outputId": "50c943b6-8459-4b7b-fe15-7b1975afa02f"
      },
      "source": [
        "val_loss_min = np.Inf\n",
        "Train_loss = []\n",
        "Val_loss = []\n",
        "\n",
        "for epoch in range(25):\n",
        "  train_loss = 0\n",
        "  model.train()\n",
        "    \n",
        "  for iteration, (data, label) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data.to(device))\n",
        "    loss = criterion(output, label.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # update running train loss \n",
        "    train_loss += loss.item() * data.shape[0]\n",
        "\n",
        "  # calculate average trainnig loss over an epoch\n",
        "  train_loss = train_loss / len(train_loader.sampler)\n",
        "  Train_loss.append(train_loss)\n",
        "\n",
        "  print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "  if (epoch+1) % 5 == 0:\n",
        "      is_training = model.training\n",
        "      val_loss = 0\n",
        "      model.eval()  # preparing model for evaluation\n",
        "      for data, label in val_loader:\n",
        "        output = model(data.to(device))\n",
        "        loss = criterion(output, label.to(device))\n",
        "        # update running validation loss \n",
        "        val_loss += loss.item() * data.shape[0]\n",
        "\n",
        "      val_loss = val_loss / len(val_loader.sampler)\n",
        "      Val_loss.append(val_loss)\n",
        "\n",
        "      if val_loss <= val_loss_min:\n",
        "        print('\\tValidation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(val_loss_min, val_loss))\n",
        "        val_loss_min = val_loss\n",
        "\n",
        "      model.train(mode=is_training)\n",
        "\n",
        "  my_lr_scheduler.step()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.255343\n",
            "Epoch: 2 \tTraining Loss: 0.292157\n",
            "Epoch: 3 \tTraining Loss: 0.214546\n",
            "Epoch: 4 \tTraining Loss: 0.154673\n",
            "Epoch: 5 \tTraining Loss: 0.185218\n",
            "\tValidation loss decreased (inf --> 1.745737). Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.099152\n",
            "Epoch: 7 \tTraining Loss: 0.080168\n",
            "Epoch: 8 \tTraining Loss: 0.076451\n",
            "Epoch: 9 \tTraining Loss: 0.066365\n",
            "Epoch: 10 \tTraining Loss: 0.056935\n",
            "\tValidation loss decreased (1.745737 --> 0.797016). Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.054669\n",
            "Epoch: 12 \tTraining Loss: 0.058766\n",
            "Epoch: 13 \tTraining Loss: 0.062057\n",
            "Epoch: 14 \tTraining Loss: 0.055392\n",
            "Epoch: 15 \tTraining Loss: 0.055231\n",
            "Epoch: 16 \tTraining Loss: 0.050710\n",
            "Epoch: 17 \tTraining Loss: 0.050913\n",
            "Epoch: 18 \tTraining Loss: 0.056479\n",
            "Epoch: 19 \tTraining Loss: 0.054514\n",
            "Epoch: 20 \tTraining Loss: 0.058140\n",
            "Epoch: 21 \tTraining Loss: 0.057808\n",
            "Epoch: 22 \tTraining Loss: 0.060773\n",
            "Epoch: 23 \tTraining Loss: 0.050613\n",
            "Epoch: 24 \tTraining Loss: 0.058174\n",
            "Epoch: 25 \tTraining Loss: 0.058319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxHS7BY27-Rz",
        "outputId": "6146ae1b-d2ad-4ea9-e98e-0ef237d6299d"
      },
      "source": [
        "print(\"Validation Loss:\", Val_loss, \"\\nTrain Loss:\", Train_loss)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Loss: [1.7457374334335327, 0.7970162630081177, 0.8788511157035828, 0.9101503491401672, 0.9285068511962891] \n",
            "Train Loss: [1.2553431611119603, 0.29215702775614394, 0.2145457958425481, 0.1546728750405136, 0.1852176160625885, 0.09915206368806903, 0.08016776560914297, 0.07645086018494301, 0.06636472358911133, 0.05693543590376714, 0.054669397791699034, 0.05876613502602087, 0.0620573439343941, 0.055391626992474303, 0.055230748708262764, 0.050709948035105604, 0.05091275594717155, 0.05647902164402549, 0.054514272675558105, 0.05813951236110158, 0.05780843008835257, 0.0607732820775977, 0.05061278526494108, 0.05817355327536723, 0.05831931635988819]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0W-9raBaADg"
      },
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgIF_8ppVLt6"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBFNBXquF4M6"
      },
      "source": [
        "model = ResNet()\n",
        "model.load_state_dict(torch.load('model.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyZbbQgOFR8S",
        "outputId": "e725f9f2-5a01-49b6-c62a-3186310ade5f"
      },
      "source": [
        "test_loss = 0.0\n",
        "num_correct = 0\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "\n",
        "  for data, target in test_loader:\n",
        "    output = model(data)\n",
        "      \n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    total_predictions += target.size(0)\n",
        "\n",
        "    correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "    y_pred = np.concatenate((y_pred, predicted.numpy()))\n",
        "    y_true = np.concatenate((y_true, target.numpy()))\n",
        "      \n",
        "  test_acc = correct_predictions / total_predictions\n",
        "  print('Test Accuracy: {:.2f}%\\n'.format(test_acc * 100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 75.80%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUV_A2pTVdSi"
      },
      "source": [
        "# Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "BtRRHcOzusjR",
        "outputId": "507cac4e-4cdd-4318-9f6a-0445796a3fa4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(Train_loss)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd222f9e710>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmlz8w71aiuS",
        "outputId": "8c2487c8-7f02-4658-b68b-129c1dfeaede"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 85, 149],\n",
              "       [  2, 388]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}